# -*- coding: utf-8 -*-
"""Copy of ai-sportcaster.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ZB3r0mQ3bCVwtNCzkx4s_sWAY7ghTdK
"""

!pip install gradio
!pip install transformers
!pip install gTTS
!pip install pydub
!pip install deep_translator
!pip install pillow
!pip install torch
from transformers import BlipProcessor, BlipForConditionalGeneration
from gtts import gTTS
from pydub import AudioSegment
from deep_translator import GoogleTranslator
from PIL import Image
import torch

# Load the model and transformer
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

def generate_caption(image):
    "Generate image description in English"
    inputs = processor(images=image, return_tensors="pt")
    caption_ids = model.generate(**inputs, max_length=30)
    return processor.decode(caption_ids[0], skip_special_tokens=True)

def translate_to_arabic(text):
    return GoogleTranslator(source="en", target="ar").translate(text)

def text_to_speech(text, language):
    file_path = "output.mp3"

    if language == "ÿßŸÑÿπÿ±ÿ®Ÿäÿ©":
        arabic_intro = "Ÿáÿ∞ÿß ŸàÿµŸÅ ŸÑŸÑÿµŸàÿ±ÿ©: "
        arabic_tts = gTTS(text=arabic_intro, lang="ar")
        caption_tts = gTTS(text=text, lang="ar")

        arabic_tts.save("arabic_intro.mp3")
        caption_tts.save("caption.mp3")

        audio1 = AudioSegment.from_mp3("arabic_intro.mp3")
        audio2 = AudioSegment.from_mp3("caption.mp3")
        combined = audio1 + audio2
        combined.export(file_path, format="mp3")
    else:
        tts = gTTS(text=text, lang="en")
        tts.save(file_path)

    return file_path

def process_image(image_path, language):
    image = Image.open(image_path).convert("RGB")
    caption_en = generate_caption(image)

    if language == "ÿßŸÑÿπÿ±ÿ®Ÿäÿ©":
        caption_ar = translate_to_arabic(caption_en)
        audio = text_to_speech(caption_ar, language)
        return caption_ar, audio
    else:
        audio = text_to_speech(caption_en, language)
        return caption_en, audio

import gradio as gr



# Use the background image inside .gradio-container
custom_css = """
.gradio-container {
    background: url('https://tacteec.com/wp-content/uploads/2017/09/2-7.jpg') no-repeat center center fixed;
    background-size: cover;
    color: white;
}

footer, .svelte-1ipelgc {
    background: transparent !important;
}

.block.svelte-1ipelgc {
    background-color: rgba(0, 0, 0, 0.6); /* dark overlay */
    border-radius: 15px;
    padding: 20px;
}
"""

iface = gr.Interface(
    fn=process_image,
    inputs=[
        gr.Image(type="filepath", label="Upload an image üì∑"),
        gr.Radio(["English", "ÿßŸÑÿπÿ±ÿ®Ÿäÿ©"], label="Language")
    ],
    outputs=["text", "audio"],
    title="Aisportcaster üéô",
    description="üì∑ Upload an image and get a short spoken description in the selected language üéß",
    css=custom_css
)

iface.launch()